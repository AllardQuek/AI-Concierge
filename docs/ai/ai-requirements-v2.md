# ğŸ“ High-Level Architecture


## Overview
```less
+-----------------------------------------+
|              Mulisa System              |
+----------------+------------------------+
                 |
        +--------v--------+         
        |  React Web App  |  <-- Client (WebRTC)
        +--------+--------+
                 |
        +--------v--------+
        |     SFU/Media    |  <-- LiveKit / mediasoup
        |     Server       |
        +--------+--------+
                 |
   +-------------v-------------+
   |  Real-Time Audio Capture  |
   | +-----------------------+ |
   | | Per-user Audio Streams| |
   | +-----------------------+ |
   +-------------+-------------+
                 |
        +--------v--------+
        |   Mulisa Core    |  <-- Cloud Agent Brain
        | (STT + LLM + TTS)|
        +--------+--------+
                 |
   +-------------v-------------+
   |    External Services      |
   | - Bing / SerpAPI Search   |
   | - Booking / Maps APIs     |
   +---------------------------+
```

## With Clients
```less
Client A (WebRTC)              Client B (WebRTC)
      |                               |
      |                               |
      |------- WebRTC Audio ----------|
                   |
                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      SFU         â”‚ â—„â”€â”€â”€â”€â”€â”€â”€ Mulisa joins as 3rd participant
         â”‚ (LiveKit /       â”‚
         â”‚  mediasoup)      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   |
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                         â–¼
Audio Stream A           Audio Stream B
       |                         |
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     STT Module      â”‚ â—„â”€â”€ Azure Speech-to-Text (16kHz audio)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    LLM Agent Core   â”‚ â—„â”€â”€ GPT-4o + context memory
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     TTS Module      â”‚ â—„â”€â”€ Azure Neural TTS (SSML support)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Audio Publisher    â”‚ â”€â”€> Injects Mulisa audio into SFU
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
          Speech Response
                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      SFU         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        /                      \
       â–¼                        â–¼
Client A hears Mulisa    Client B hears Mulisa
```

---

## ğŸ‘¥ How Mulisa Joins the Call

### âœ… TL;DR:

Mulisa is implemented as a **virtual WebRTC participant** that:

* Connects to the same SFU as the human users
* Negotiates media tracks like any peer (via ICE/SDP)
* Publishes audio (TTS) and optionally receives audio streams from others (for STT)

---

## ğŸ” Detailed Breakdown

### 1. **Mulisa = WebRTC Client**

* Mulisa has her own WebRTC connection, just like a human user.
* She runs a **headless client** (Node.js, Go, or browser-like wrapper) that:

  * Connects to the SFU
  * Performs full **SDP offer/answer exchange**
  * Negotiates ICE candidates
  * Sets up DTLS/SRTP media channels

âœ… Yes â€” **Mulisa needs to do ICE candidate exchange** to establish media transport paths.

---

### 2. **Audio Routing via SFU**

* Once joined, SFU treats Mulisa as a **normal participant**:

  * Other participants publish their audio to the SFU
  * Mulisa subscribes to both audio streams (optionally)
  * When Mulisa responds, she publishes her own audio stream (TTS output)

### ğŸ§ Inbound:

Mulisa receives streams from A and B for transcription.
These are:

* Optionally mixed by SFU (for simplicity)
* Or kept as individual tracks (for diarization and speaker separation)

### ğŸ”Š Outbound:

Mulisa publishes her synthesized voice (TTS result) as a normal audio track:

* SFU distributes this to A and B as if it's from a third user.

---

### 3. **Authentication & Signaling**

* Mulisa authenticates using API keys or tokens (e.g. JWT for LiveKit)
* She joins rooms via signaling just like other users:

  * Room join â†’ offer/answer â†’ ICE â†’ audio flow
* Can be auto-joined programmatically upon invocation ("Hey, Mulisa")

---

## ğŸ›  Example: LiveKit Join Flow

1. Mulisa makes a `joinRoom()` call with a server-issued JWT.
2. SFU (LiveKit) sends an SDP offer.
3. Mulisa answers with her capabilities (audio only, maybe no video).
4. ICE negotiation completes (needs STUN/TURN like other clients).
5. Mulisa subscribes to tracks from A and B.
6. On LLM response, Mulisa publishes audio track â†’ SFU sends to A & B.

---

## ğŸ“¦ What You Need to Implement Mulisa as a Virtual Participant

| Component               | Notes                                                                                                                                   |
| ----------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| WebRTC client (Node.js) | [node-webrtc](https://github.com/node-webrtc/node-webrtc), [mediasoup-client](https://mediasoup.org/documentation/v3/mediasoup-client/) |
| SFU (e.g., LiveKit)     | Provides media server infra and APIs                                                                                                    |
| TURN server             | Required in real-world mobile networks                                                                                                  |
| TTS pipeline            | Audio must be encoded (e.g., Opus) and injected into MediaStreamTrack                                                                   |
| Audio bridge/buffer     | Receives streams from SFU â†’ STT pipeline                                                                                                |
| Signaling               | Use LiveKit SDK or custom signaling to manage join/leave                                                                                |

---

## ğŸ” Bonus: How Mulisa Knows When to Join

Trigger mechanisms:

* Wake word detection from in-call audio (â€œHey Mulisaâ€)
* UI button click on either participantâ€™s side
* External control (e.g., backend server command)

Once triggered:

* Backend authorizes + spins up Mulisa client
* Client joins via WebRTC into SFU
* Begins subscribing + publishing audio

---

Great follow-up â€” letâ€™s talk through why using **your current direct WebRTC peer-to-peer setup** (i.e. no SFU, just `RTCPeerConnection` between A and B) is **not ideal** for integrating an agent like Mulisa, and why an SFU is recommended.

---

## ğŸš« Why Pure P2P WebRTC Isn't Ideal for Mulisa

### 1. **WebRTC Mesh = Full-Mesh Complexity**

If A â†” B is a direct P2P call, and you now want to add Mulisa (C), youâ€™ll need:

* A â†” B
* A â†” Mulisa
* B â†” Mulisa

Thatâ€™s **3 peer connections** and a **mesh topology**, which:

* Doesnâ€™t scale beyond 2â€“3 users
* Adds complexity for audio routing
* Requires **each participant to encode/send to N peers**
* Has **NÃ—N connections**, vs N with SFU

---

### 2. **Audio Routing Becomes Manual**

In P2P:

* You must explicitly forward audio from A and B to Mulisa
* Then synthesize Mulisaâ€™s voice and forward that separately to A and B
* There is **no central router** to control who hears what

âš ï¸ This creates:

* Sync issues (you need to carefully align timestamps)
* Potential echo/resends
* Increased bandwidth on each client

---

### 3. **Adding/Removing Participants Dynamically is Painful**

You canâ€™t "join" someone to a P2P call without:

* Negotiating a **new peer connection** from scratch
* Restarting ICE (which is disruptive and often causes brief disconnects)
* Handling re-negotiation logic manually

With SFU:

* Mulisa just **joins the room**
* Audio is routed transparently

---

### 4. **Client Bandwidth & Battery Load**

In P2P:

* Each user must send 2 outgoing audio streams (to other peer and to Mulisa)
* On mobile devices (especially 4G/5G), this can kill battery and fail under NAT

With SFU:

* Each user sends **1 upstream**
* And receives only whatâ€™s needed (A, B, Mulisa)

---

## âœ… Why SFU Solves These

| Problem                      | P2P (mesh)         | SFU                   |
| ---------------------------- | ------------------ | --------------------- |
| Dynamic participant joining  | Hard               | Easy                  |
| Audio mixing / sync          | Manual             | Handled centrally     |
| Bandwidth scaling            | Poor               | Excellent             |
| Low latency                  | Depends on network | Consistently low      |
| Transcription from one point | You must route     | SFU streams to Mulisa |
| Third party audio injection  | Hacky              | Publish track to SFU  |

---

## ğŸ”„ Still Want to Stick with P2P?

You *can* hack together something like:

1. A & B both send audio to your backend (via WebRTC insertable streams or Web Audio)
2. Backend does STT + LLM + TTS
3. Backend sends audio back **via a separate `MediaStreamTrack`** (not trivial)
4. Inject that track manually into A & B

âš ï¸ But:

* Youâ€™ll need to handle re-negotiation every time
* Youâ€™ll double the work on client side
* It gets **extremely brittle**

---

### âœ… Recommendation

If you're serious about:

* Multi-party audio (even just 2+1 like A, B, Mulisa)
* Scalability
* Minimal latency
* Easy maintenance

**Use an SFU (like LiveKit)**. Even their free tier or local Docker image is easy to prototype with and way more production-suited.
